---
title: 'STA 380 Homework 2 Mulcahy, Quijano, Reynolds'
author: "Daniel Quijano"
date: "August 17, 2017"
output:
  md_document:
    variant: markdown_github
---

##1. Flights at ABIA
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
abia = read.csv('http://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv', header=TRUE)
abia = subset(abia, abia$Origin == "AUS")
#Getting only late flights
late_flights = subset(abia, abia$DepDelay > 0)

```


What kind of analysis can we perform that actually produces actionable results for a traveler. For example, say you have a wedding or you are attending a conference in Atlanta. The decision of where you are going is fixed and there is a small degree of flexibilty in terms of when you can fly. Therefore, finding information like average delay from Austin to Atlanta would not be relevant, because regardless as to the delay, you have to go to Atlanta.

The assumptions we will make, then, are that the traveler can not be flexible in terms of where  they go, but they can be reasonably flexible in terms of when they fly. 

First, let's try to get an idea about what time of day the traveler should try to fly by looking at how often flights are delayed at certain times of day:

```{r}
library(lattice)
library(dplyr)
delay_histo = histogram(late_flights$DepTime/100, main = 'Late Flights Departure Times', xlab = 'Hour', breaks = 24)

delay_histo

```
From this histogram, we see a normalized histogram displaying the number of total delayed flights that depart at certain times of the day. In general, 11:00 a.m. to 8:00 p.m. are the times with the highest proportion of delayed flights. This seems reasonable as congestion on the runway or near the gate may affect other flights at busy times of the day.

Now that we know at what time of day delays are more likely, let's see how long you can expect to wait at certain times of the day (all flights, not just delayed ones).
```{r}

plot(cut(abia[abia$DepDelay >0,]$CRSDepTime/100, seq(0, 24 , by = 1), labels = 1:24), abia[abia$DepDelay > 0,]$DepDelay, xlab = 'Hour', ylab = 'Delay', main = 'Expected Delay at Certain Time of Day', ylim = range(0:200))

```
This plot shows us that the median flight delay is not dependent on time of day. 

Now let's see how the average wait time looks for all flights compared to delayed ones.
```{r}
par(mfrow=c(1,2))
#plot of average delay time by scheduled departure hour
plot(tapply(abia$DepDelay, cut(abia$CRSDepTime, seq(0, 2400, by=100)), mean, na.rm = TRUE), ylim=range(0,60), xlab = 'Hour', ylab = "Expected Delay", main = 'All Flights')
#plot of average delay time by scheduled departure hour (excluding ontime)
plot(tapply(abia[abia$DepDelay > 0,]$DepDelay, cut(abia[abia$DepDelay > 0,]$CRSDepTime, seq(0, 2400, by=100)), mean, na.rm = TRUE), ylim=range(0,60), xlab = 'Hour', ylab = "Expected Delay", main = 'Late Flights')

```
Here, we can see that the average amount of time you can expect to wait increases by about 15-20 minutes if you find out your flight is delayed. for this situation, it is probably more useful to use the mean because there do not appear to be a large number of outliers that could be inflating the mean.

Our lesson would be to travel early in the day. There are fewer delayed flights and they are not delayed as much. 


## 2. Author attribution


```{r}
library(tm)
library(e1071)
library (plyr)
library(caret)

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

```

 
```{r}
author_dirs = Sys.glob('/Users/christinemcallister/Documents/Summer Semester/Predictive Models/Part2/STA380-master/data/ReutersC50/C50train/*')
file_list = NULL
labels = NULL
x = nchar('/Users/christinemcallister/Documents/Summer Semester/Predictive Models/Part2/STA380-master/data/ReutersC50/C50train/*')
for(author in author_dirs) {
  author_name = substring(author, first=x)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}
```


```{r}
#Apply readerPlain to file_list, name columns based on names file_list
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
my_corpus = Corpus(VectorSource(all_docs))

```


```{r}
#Preprocessing & creating DTM
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM = DocumentTermMatrix(my_corpus)
DTM 

class(DTM)  

inspect(DTM[1:10,1:20])
DTM = removeSparseTerms(DTM, 0.975) # only want words that are in 97% of docs
DTM

```


```{r}
#Now for test data:
author_dirsTest = Sys.glob('/Users/christinemcallister/Documents/Summer Semester/Predictive Models/Part2/STA380-master/data/ReutersC50/C50test/*')
file_listTest = NULL
labelsTest = NULL
xT = nchar('/Users/christinemcallister/Documents/Summer Semester/Predictive Models/Part2/STA380-master/data/ReutersC50/C50test/*')
for(author in author_dirsTest) {
  author_name = substring(author, first=xT)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_listTest = append(file_listTest, files_to_add)
  labelsTest = append(labelsTest, rep(author_name, length(files_to_add)))
}
length(file_listTest)


all_docsTest = lapply(file_listTest, readerPlain) 
names(all_docsTest) = file_listTest
names(all_docsTest) = sub('.txt', '', names(all_docsTest))

my_corpusTest = Corpus(VectorSource(all_docsTest))
```

Preprocessing & creating Test DTM
```{r}
my_corpusTest = tm_map(my_corpusTest, content_transformer(tolower)) # make everything lowercase
my_corpusTest = tm_map(my_corpusTest, content_transformer(removeNumbers)) # remove numbers
my_corpusTest = tm_map(my_corpusTest, content_transformer(removePunctuation)) # remove punctuation
my_corpusTest = tm_map(my_corpusTest, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpusTest = tm_map(my_corpusTest, content_transformer(removeWords), stopwords("SMART"))

# Deal with words that weren't in train set:
freqTrain = findFreqTerms(DTM, 1) # use only words that have appeared in DTM (train)
length((freqTrain)) # 1411 -> same as number terms in DTM train

DTMtest = DocumentTermMatrix(my_corpusTest, control=list(dictionary = freqTrain))
DTMtest 

class(DTMtest)

inspect(DTMtest[1:10,1:20])
```

## We will try Naive Bayes and Random Forests


```{r}
#First, Naive Bayes:
#Boolean feature Multinomial Naive Bayes
convert_count = function(x) {
  y = ifelse(x>0, 1, 0)
  y = factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y}
```


```{r}
#Apply the convert_count function to get final training and testing DTMs
trainNB = apply(DTM, 2, convert_count)
trainNB = as.matrix(trainNB)
row.names(trainNB) = file_list
trainNB = as.data.frame(trainNB,stringsAsfactors = FALSE)
trainNB$answer = as.factor(labels)

testNB = apply(DTMtest, 2, convert_count)
testNB = as.matrix(testNB)
testNB = as.data.frame(testNB,stringsAsfactors = FALSE)
row.names(testNB) = file_listTest
testNB$answer = as.factor(labelsTest)
```


```{r}
#Train the classifier
classifier = naiveBayes(answer ~ ., data = trainNB, laplace = 1)  

# Use the NB classifier we built to make predictions on the test set.
#pred = predict(classifier, testNB)

# Create a truth table by tabulating the predicted class labels with the actual class labels 
# table("Predictions" = pred,  "Actual" = testLabels)
# 
# conf.mat = confusionMatrix(pred, labelsTest)
# 
# conf.mat$byClass
# conf.mat$overall
# conf.mat$overall['Accuracy']
```



Now we move on to Random Forests
```{r RF}
library(randomForest)


#set up train and test datasets using Xtrain and Xtest
Xtrain = as.matrix(DTM) # Dense matrix
TRdataset <- as.data.frame(Xtrain)
TRdataset$answers <- labels #adding a column for the authors
TRdataset$answers = factor(TRdataset$answers) # Encoding the target feature as factor
Xtest = as.matrix(DTMtest) # Dense matrix
TEdataset <- as.data.frame(Xtest)
TEdataset$answers <- labelsTest #adding a column for the authors
TEdataset$answers = factor(TEdataset$answers) # Encoding the target feature as factor

rfcol = ncol(TRdataset)

#train the RF model
RF <- randomForest(x = TRdataset[-rfcol], y = TRdataset$answers, ntree = 10)
#use the RF model to predict on the test set
y_pred <- predict(RF, newdata = TEdataset[-rfcol])



#create a confusion matrix
conf <- RF$confusion
#show our errors errors for each author
RF$confusion[, 'class.error']


cm <- table(TEdataset[,rfcol], y_pred)
cm['EricAuchard',]
```

The class errors shown explain our accuracy at predicting each author using the random forest model. Our model best predicts articles for authors like JimGilchrist, LynneO'Donnell, and LydiaZajc (their error rates are 20%, 24%, and 28%, respectively). Our model's worst predictions are for EricAuchard with 79% error. EricAuchard is often misclassified as SamuelPerry (9 out of 50 times).




## Q3:Practice with association rule mining

```{r load_libraries}
library(tm) 
detach(package:tm, unload=TRUE)
library(arules)  
library(splitstackshape)
```

```{r get_data_ready}
#read in the data
groceries_raw <- read.csv('http://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt', sep='\n', header=FALSE)

#head(groceries_raw,12)

#names the basket column
groceries_raw$basket = rownames(groceries_raw)
#split the string of items into individual rows
groceries = cSplit(groceries_raw, "V1", direction='long', ",")
#look at the first few items
head(groceries)

# Turn basket into a factor
groceries$basket <- factor(groceries$basket)

# First create a list of baskets: vectors of items by consumer
# apriori algorithm expects a list of baskets in a special format
# In this case, one "basket" of items per basket
# First split data into a list of items for each basket
baskets <- split(x=groceries$V1, f=groceries$basket)

## Remove duplicates ("de-dupe")
baskets <- lapply(baskets, unique)

## Cast this variable as a special arules "transactions" class.
baskettrans <- as(baskets, "transactions")
```

```{r run_apriori}
# Now run the 'apriori' algorithm
basketrules <- apriori(baskettrans, parameter=list(support=.01, confidence=.4, maxlen=4))

# Look at the output
inspect(basketrules)
```

This list is our list of rules with support>.01, confidence>.4, and no more than 4 items per set. We see that "staple" items like milk and vegetable are most often predicted. This makes sense because with a support level of 1%, there will be very few combinations that are present in enough baskets to make it into our list with high confidence. Only the staple items are bought together frequently enough to qualify.

```{r tune}
#re-run rules
basketrules <- apriori(baskettrans, parameter=list(support=.004, confidence=.4, maxlen=4))

## Choose a subset
inspect(subset(basketrules, subset=lift > 5))
```
When we re-run the apriori rules with support lowered to .4%, we see many more rules involving items that aren't necessarily considered "staple" items. Some of these rules have a much higher lift since an item that is less common in the overall basket list can be predicted with higher confidence given some other items in the basket. For instance, we see that bottled beer is 5 times more likely to be in a basket given that the basket also contains liquor.


